import psycopg2
import requests
import os
import json
import asyncio
import argparse
from typing import List
from bs4 import BeautifulSoup
import re
from datetime import date
from dotenv import load_dotenv

load_dotenv()  # To obtain keys from the .env file

# ----------------- FOR CLI LIBRARY COMMAND -----------------


def extract_command_value() -> str:
    """Returns the value of type str prompted in the command line following --days-needed"""

    parser = argparse.ArgumentParser(
        description="Custom command for specifying days for fetching jobs."
    )

    # Add an argument (the custom command) along with the help functionality to see what the command does
    parser.add_argument(
        "--days-needed", type=str, help="The number of days needed to fetch jobs."
    )

    # Parse the argument and insert into a variable
    arguments = parser.parse_args()

    # Create a new variable to access the --days-needed command
    days_needed_variable = arguments.days_needed

    # Return the value from the --days-needed custom command
    return days_needed_variable


# ----------------- FOR POSTGRESS -----------------


db_uri = os.getenv("DB_URI")
table_name = os.getenv("DB_TABLE")


def instantiates_db_connection():
    """Returns the connection from the DB"""

    return psycopg2.connect(db_uri)


def create():
    """Creates the DB. Only needs to be called once."""

    with instantiates_db_connection() as connection:
        cursor = connection.cursor()

        cursor.execute(
            f"""CREATE TABLE IF NOT EXISTS {table_name}(_company TEXT, _title TEXT, _location TEXT, _link TEXT, _processed INTEGER DEFAULT 0)"""
        )

        connection.commit()


def ingest_opportunities(job_data):
    """Inserts opportunities if and only if they do not already exist"""

    with instantiates_db_connection() as connection:
        cursor = connection.cursor()

        for job in job_data:
            cursor.execute(
                f"SELECT * FROM {table_name} WHERE _company = %(_company)s AND _title = %(_title)s AND _location = %(_location)s",
                {
                    "_company": job["_company"],
                    "_title": job["_title"],
                    "_location": job["_location"],
                },
            )
            row = cursor.fetchone()

            if row is None:
                cursor.execute(
                    f"INSERT INTO {table_name} (_company, _title, _location, _link, _processed) VALUES (%s, %s, %s, %s, %s)",
                    (job["_company"], job["_title"], job["_location"], job["_link"], 0),
                )
        connection.commit()


# ----------------- JOB DATA -----------------


def request_rapidapi_indeed_data() -> List[object]:
    """
    This API call retrieves a formatted response object
    and returns a List[object] as the result
    """

    url = os.getenv("RAPID_API_URL")

    rapid_api_key = os.getenv("RAPID_API_KEY")
    rapid_api_host = os.getenv("RAPID_API_HOST")

    headers = {
        "X-RapidAPI-Key": rapid_api_key,
        "X-RapidAPI-Host": rapid_api_host,
    }

    rapid_jobs = []
    response = requests.get(url, headers=headers).json()

    command_line_value = extract_command_value()  # Extracts command-line value

    for elem in response["hits"]:
        time = elem["formatted_relative_time"]

        numeric = re.search(r"\d+", time)
        formatted_time_integer = int(numeric.group()) if numeric else 0

        if len(rapid_jobs) <= 5 and int(command_line_value) >= formatted_time_integer:
            job = {}
            job["_company"] = elem["company_name"]
            job["_title"] = elem["title"]
            job["_location"] = elem["location"]
            job["_link"] = f'https://www.indeed.com/viewjob?jk={elem["id"]}&locality=us'

            rapid_jobs.append(job)

    return rapid_jobs


def request_linkedin_data() -> List[object]:
    """Returns a List[object] which contains web scraped job content"""

    url = os.getenv("LINKEDIN_URL")

    response = requests.get(url)

    content = response.text

    parse_content = BeautifulSoup(content, "html.parser")

    div_post = parse_content.find_all(
        "div",
        class_="base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card",
    )

    date_div_post = parse_content.find_all(
        "time",
        class_="job-search-card__listdate",
    )  # Recieves all the job posting dates

    command_line_value = extract_command_value()  # Extracts command-line value

    linked_in_jobs = []

    for elem, date in zip(div_post, date_div_post):
        """
        If the value of date.text.strip()
        has the word "days" in it, we access the number,
        and compare it with the value of the command line
        """

        if "days" in date.text.strip():
            numeric = re.search(r"\d+", date.text.strip())
            formatted_time_integer = int(numeric.group()) if numeric else 0

            if (
                len(linked_in_jobs) <= 5
                and int(command_line_value) >= formatted_time_integer
            ):
                job = {}
                job["_company"] = elem.find(
                    "h4", class_="base-search-card__subtitle"
                ).text.strip()
                job["_title"] = elem.find(
                    "h3", class_="base-search-card__title"
                ).text.strip()
                job["_location"] = elem.find(
                    "span", class_="job-search-card__location"
                ).text.strip()
                job["_link"] = elem.find("a", class_="base-card__full-link")[
                    "href"
                ].split("?")[0]

                linked_in_jobs.append(job)
    return linked_in_jobs


# ----------------- HELPER FUNCTIONS -----------------


def list_all_opportunities() -> List[object]:
    """Lists all oppportunities in DB"""

    with instantiates_db_connection() as connection:
        cursor = connection.cursor()

        cursor.execute(f"SELECT * FROM {table_name}")

        rows = cursor.fetchall()

        for row in rows:
            _company, _title, _location, _link, _processed = row

            print("Company:", _company)
            print("Title:", _title)
            print("Location:", _location)
            print("Link:", _link)
            print("Processed:", _processed)
            print(" ")

        connection.close()


def list_filtered_opportunities() -> List[object]:
    """Returns a List[object] of job data that have a status of _processed = 0"""

    with instantiates_db_connection() as connection:
        cursor = connection.cursor()

        cursor.execute(f"SELECT * FROM {table_name} WHERE _processed = 0 LIMIT 10")
        rows = cursor.fetchall()

        not_processed_rows = []

        for row in rows:
            job = {}

            job["_company"] = row[0]
            job["_title"] = row[1]
            job["_location"] = row[2]
            job["_link"] = row[3]
            job["_processed"] = row[4]

            # Uncomment to view jobs up to 5 that have not been posted

            # print("Company:", row[0])
            # print("Title:", row[1])
            # print("Location:", row[2])
            # print("Link:", row[3])
            # print("Processed:", row[4])
            # print(" ")

            not_processed_rows.append(job)

    return not_processed_rows


def format_opportunities(data_results) -> str:
    """Recieves data from list_filtered_opporunities() and returns a single string message"""

    formatted_string = ""

    for data_block in data_results:
        _company = data_block["_company"]
        _title = data_block["_title"]
        _location = data_block["_location"]
        _link = data_block["_link"]

        formatted_string += f"[**{_company}**]({_link}): {_title} `@{_location}`!\n"

    return formatted_string


def update_opportunities_status(data_results):
    """Updates the status of the jobs to _processed = 1 after it's been sent by the discord bot"""

    with instantiates_db_connection() as connection:
        cursor = connection.cursor()

        for data_block in data_results:
            cursor.execute(
                f"UPDATE {table_name} SET _processed = %s WHERE _company = %s  AND _title = %s  AND _location = %s",
                (
                    1,
                    data_block["_company"],
                    data_block["_title"],
                    data_block["_location"],
                ),
            )

        connection.commit()


# ----------------- RESET FUNCTION (DEBUGGING PURPOSES) -----------------


def reset_processed_status():
    """Jobs status will be set to _processed = 0 for testing a debugging purposes"""

    with instantiates_db_connection() as connection:
        cursor = connection.cursor()

        cursor.execute(
            f"SELECT _company, _title, _location FROM {table_name} WHERE _processed = 1"
        )

        rows = cursor.fetchall()

        for row in rows:
            _company, _title, _location = row[:3]

            cursor.execute(
                f"UPDATE {table_name} SET _processed = 0 WHERE _company = %s AND _title = %s AND _location = %s",
                (_company, _title, _location),
            )

        connection.commit()


# ----------------- DISCORD BOT -----------------


async def execute_opportunities_webhook(webhook_url, message):
    """
    Executes the message which recieves the formatted message
    from the format_opportunities() function as well as the webhook
    url for the respected discord channel
    """

    # Create a dictionary payload for the message content
    payload = {
        "content": " # ✨ :capysmart: NEW JOB POSTINGS BELOW! :capycool: ✨ ",
        "tts": False,
        "embeds": [
            {
                "title": f"> Job Postings for {date.today()}\n\n\n\n",
                "description": message,
                "color": 0x05A3FF,
            }
        ],
    }

    # Convert the payload to JSON format
    json_payload = json.dumps(payload)

    # This will send a POST request to the webhook_url!
    response = requests.post(
        webhook_url, data=json_payload, headers={"Content-Type": "application/json"}
    )

    if response.status_code == 204:
        print("Webhook message was sent sucessfully!")
    else:
        print(f"Failed to send webhook message. Status Code: {response.status_code}")


# ----------------- EXECUTE FUNCTIONS -----------------

rapid_data = request_rapidapi_indeed_data()
linkedin_data = request_linkedin_data()

ingest_opportunities(rapid_data)
ingest_opportunities(linkedin_data)


"""
To test the code without consuming API requests, call reset_processed_status().
This function efficiently resets the processed status of all job postings by setting them to _processed = 0.
By doing so, developers can run tests without wasting valuable API resources.
To do so, please uncomment the following line of code: 
"""
# reset_processed_status()


data_results = list_filtered_opportunities()
formatted_message = format_opportunities(data_results)


async def main():
    discord_webhook = os.getenv("DISCORD_WEBHOOK")

    await execute_opportunities_webhook(discord_webhook, formatted_message)

    update_opportunities_status(data_results)


if __name__ == "__main__":
    asyncio.run(main())
